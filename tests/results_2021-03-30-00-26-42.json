{
    "amazon_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "10th_percentile_acc": 0.4000000059604645,
            "acc_avg": 0.7116081118583679
        },
        "val_eval": {
            "10th_percentile_acc": 0.4000000059604645,
            "acc_avg": 0.7005497217178345
        },
        "id_test_eval": {
            "10th_percentile_acc": 0.4000000059604645,
            "acc_avg": 0.7152289748191833
        },
        "test_eval": {
            "10th_percentile_acc": 0.375,
            "acc_avg": 0.6947526335716248
        }
    },
    "camelyon17_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "acc_avg": 0.9594755768775941
        },
        "val_eval": {
            "acc_avg": 0.8581662178039551
        },
        "test_eval": {
            "acc_avg": 0.6536155343055725
        }
    },
    "civilcomments_ERM_2021-03-30-00-26-42": {
        "val_eval": {
            "acc_wg": 0.5,
            "acc_avg": 0.91921204328537
        },
        "test_eval": {
            "acc_wg": 0.4462809860706329,
            "acc_avg": 0.914635956287384
        }
    },
    "iwildcam_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "F1-macro_all": 0.4372776948830315,
            "acc_avg": 0.7893297076225281
        },
        "val_eval": {
            "F1-macro_all": 0.2974963439077729,
            "acc_avg": 0.5768716335296631
        },
        "id_test_eval": {
            "F1-macro_all": 0.387449478373176,
            "acc_avg": 0.7092024683952332
        },
        "test_eval": {
            "F1-macro_all": 0.2564969999778209,
            "acc_avg": 0.7218976616859436
        }
    },
    "ogb-molpcba_ERM_2021-03-30-00-26-42": {
        "val_eval": {
            "ap": 0.18442854531627065
        },
        "test_eval": {
            "ap": 0.17555657122011672
        }
    },
    "poverty_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "r_wg": 0.4764947851631088,
            "r_all": 0.7761079758032177
        },
        "val_eval": {
            "r_wg": 0.4456415426893632,
            "r_all": 0.7598417226399689
        },
        "id_test_eval": {
            "r_wg": 0.6223843555371653,
            "r_all": 0.8066724585844707
        },
        "test_eval": {
            "r_wg": 0.3375143966571925,
            "r_all": 0.7853469539703519
        }
    },
    "fmow_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "acc_worst_region": 0.33834585547447205,
            "acc_avg": 0.36149826645851135
        },
        "val_eval": {
            "acc_worst_region": 0.3611111044883728,
            "acc_avg": 0.392570286989212
        },
        "id_test_eval": {
            "acc_worst_region": 0.3734939694404602,
            "acc_avg": 0.3989408612251282
        },
        "test_eval": {
            "acc_worst_region": 0.21969696879386905,
            "acc_avg": 0.3500678539276123
        }
    },
    "py150_ERM_2021-03-30-00-26-42": {
        "id_val_eval": {
            "acc": 0.6802120208740234,
            "Acc (Overall)": 0.6876065135002136
        },
        "val_eval": {
            "acc": 0.6499795317649841,
            "Acc (Overall)": 0.6613532900810242
        },
        "id_test_eval": {
            "acc": 0.6828336715698242,
            "Acc (Overall)": 0.6846407055854797
        },
        "test_eval": {
            "acc": 0.6450059413909912,
            "Acc (Overall)": 0.6634054780006409
        }
    },
    "civilcomments_reweighted_2021-03-30-00-26-42": {
        "val_eval": {
            "acc_wg": 0.5909090638160706,
            "acc_avg": 0.8975210189819336
        },
        "test_eval": {
            "acc_wg": 0.5619834661483765,
            "acc_avg": 0.8952758312225342
        }
    },
    "civilcomments_groupDRO_2021-03-30-00-26-42": {
        "val_eval": {
            "acc_wg": 0.6216216087341309,
            "acc_avg": 0.8727312684059143
        },
        "test_eval": {
            "acc_wg": 0.5919002890586853,
            "acc_avg": 0.8656749725341797
        }
    },
    "civilcomments_deepCORAL_2021-03-30-00-26-42": {
        "val_eval": {
            "acc_wg": 0.6276595592498779,
            "acc_avg": 0.8554670214653015
        },
        "test_eval": {
            "acc_wg": 0.585669755935669,
            "acc_avg": 0.8449693322181702
        }
    },
    "civilcomments_IRM_2021-03-30-00-26-42": {
        "val_eval": {
            "acc_wg": 0.6329787373542786,
            "acc_avg": 0.8756086826324463
        },
        "test_eval": {
            "acc_wg": 0.5950413346290588,
            "acc_avg": 0.8722529411315918
        }
    }
}